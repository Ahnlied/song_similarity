#+TITLE: Methodology to determine song similarity
#+AUTHOR: Jorge Andrés Carballo Santana
#+DATE: september 2023

#+BEGIN_COMMENT
#+BIBLIOGRAPHY: bibliography.bib
#+END_COMMENT

The main objective of this project is to extract non-trivial features which describe a song. Ideally we can run this script with every song available, determine their score for every feature and, once we have a good database, create a song similarity score to recommend songs and create playlist with similar songs.


[By trivial I mean parameters that doesn't have a formal musical justification and have a rather emotional or abstract definition (danceability, mood, energy, ...)]


There are some parameters that characterizes a song, such as:

** 1. TONE COLOR/TIMBRE/INSTRUMENTATION
   - percussion = instr. which is struck, (drums, bell, rattle, marimba, vibes, xylophone, glockenspiel, piano (because note sounded by striking hammer))
   - strings (violin, viola, cello, double bass, guitar, harp etc.)
   - brass (trumpet, trombone, French horn, tuba, etc.)
   - woodwinds (flute, oboe, clarinet, bassoon, saxophone, etc. – last 4 = reeds, ob. & bsn. = double reeds vs. clar. & sax. = single reeds)
   - voices (soprano, alto, tenor, bass, etc. or the medieval triplum, duplum, tenor)
   - a.d.s.r. = attack-decay-sustain-release, a technical description of tone color, e.g. for synthesizer sound envelope design or analysis
     
** 2. RHYTHM
   - beat/pulse
   - meter = group of beats, e.g. group of four beats–"4/4"–numerator is number of beats
   - duple meter = a group of beats divisible by 2
   - triple meter = a group of beats divisible by 3
   - subdivision of individual beat can be duple or triple as well
   - a duple meter with triple subdivision is called compound duple meter and, vice versa, compound triple meter has duple subdivision

** 3. MELODY:
   - pitch + time = melody
   - pitch: high/low = sound wave, frequency of vibration of air on eardrum
   - melodic contour = a draw-able shape which shows perceived pitch height and rate of change
   - phrases–cadence–climax–scales (scales also pertain to harmony)


** 4. DYNAMICS/EXPRESSION:
   - loud/soft (forte/piano)
   - legato/staccato (smooth/choppy or spiky), etc.

** 5. TEXTURE (# OF INDEPENDENT PARTS):
   - monophony (one melody)
   - homophony (one main melody
   - polyphony (many simultaneous melodies which seem to relate)

** 6. HARMONY:
   - consonant/consonance (pleasing) vs. dissonant/dissonance (jarring, tense, ugly, unsettling)
   - cadence = ending, punctuation, as in speech
   - tonal system vs. modal or pre-tonal, atonal, microtonal or post-tonal
   - notes in scale or melody which play important harmonic roles
   - tonic (home note)
   - dominant (5 steps above tonic)
   - subdominant (5 steps below tonic)


** 7. FORM:
   - structure; music as architecture in space and time; musical similarity leads to symmetry
   - large-scale repetition and contrast diagrammed as rhyme scheme, e.g. 'ABA' represents a form which returns to the opening phrase before closing (also called an arch or da capo form)
   - strophic - 'AAA..etc.' like a song with verses, or a hymn
   - through-composed - a form where repetition is indiscernible or overly intricate


** 8. TEXT/EXTRAMUSICALITY:
   - use of words or other nonmusical ideas as essential part of piece vs. "absolute" music


* Approach:
The first approach is to develop either Machine Learning model or a numerical approach for each identification parameter of a song, such as, when executed each algorithm on a given song, the output would be a list of attributes given by each identification parameter. Our objective is to create a big database that contains the attributes of each
existing song.

The application of this model would be a recommendation system which brings fidelity to the musical taste of the user. The recommendation system would be built
by the intersection of attributes, and maybe other modifiable parameters like "range of similarity", the message transmitted by the song's lyrics and so on.

** 1. Develpoment TONE COLOR/TIMBRE/INSTRUMENTATION
The approach that we followed to obtain this feature is to generate time windows and extract 13 Mel-Frequency Cepstrum Coefficients (MFCC), root mean square (rms), spectral centroid (spec_cent), spectral bandwidth (spec_bw), spectral rolloff, zero crossing rate (zcr) [cite:@feat_from_audio]. With these information we can detect to which instrument the past features correspond.

For each time window first we determine the type of instrument the prior corresponds, with that we determine the specific instrument. The instruments that we consider are separated in the type of instrument and the following:

   - Woodwind: Clarinet, Flute, Oboe, Bassoon, Alto Saxophone, Wind Instrument
   - Brass: Bass Tuba, French Horn, Trombone, Trumpet in C
   - Bowed string instruments: Cello, Viola, Violin, Bowed string instrument
   - Plucked string instruments: Guitar, Electric guitar, Acoustic guitar, Tapping, Bass guitar, Contrabass
   - Keyboard: Keyboard, Accordion
   - Percussion: Steelpan,Percussion, Drum and bass
   - Singing: Single Voice Singing, Group Singing
   - Noise: Noise

For these categories there exist an open-source database with single notes audio for each instrument [11]. Other database used is Google audioset [8], which provides links of YouTube videos with content provided by a variety of tags.

#+BEGIN_COMMENT
[cite:@tinysol]
[cite:@audioset] 
#+END_COMMENT

Currently the result is a frequential count, but the objective is to create an statistical, probabilistic or a more complex result.




** 2. and 3. Develpoment RYTHM and MELODY:

To determine the melodic contour [3,4,10] our approach is to, as the previous section, split the whole song in time windows and determine the pitch of each frame which will characterize the melodic contour.

#+BEGIN_COMMENT
[cite:@melody_transcription; @spectral_envelope; @melodic_contour]
#+END_COMMENT

The database utilized to train the models [6,11] which are note-separated sounds of individual instruments, in order to extract the corresponding frequencies and harmonics.

#+BEGIN_COMMENT
 [cite:@nsynth_database; @tinysol]
#+END_COMMENT

Right now,  the melodic contour works as a mere finger print of the song; to extract the corresponding features of each song finger-print we expect to develop 

Also, it may be important to point out that the current melodic contour strategy has static frequencies (which range in the audible spectrum 200-4000Hz) which will describe the features. In the future it will be nice to stablish a range of dynamic relevant frequencies that constitute the signal and then split the relevant features taking these range into consideration (ex: an overall high pitched song should only have features ranging from 1050Hz or higher and despise low frequencies).




* Modularity:
- Make a script that creates a data set of an input Youtube link. Make it so it can run in a terminal (this one is to run with the final model).
- Create another similar script which takes a reference Youtube link and append its output to the training dataset.

  
* Noise reduction:
- Find relations between original signal and noise. Simulate noise and add it to the single notes played.
  It would be nice to insolate and identify the instrument from a whole song an identify the signal-noise ratio.
- With this we would be able to isolate the peaks from the instrument from the background noise.
- Separate the song in parts: Background (repeating sounds which only stablishes rythm and tempo), Frontground (relevant sounds which helps on distinguish the melody of a song),
  also it would be important to retreave only singer's voices to make analysis on its characteristics. From this distinction it would be easier to extract the rythm of a song.
  

* BIBLIOGRAPHY:
- [1] /Acoustic Descriptors for Characterization of Musical Timbre Using the Fast Fourier Transform./ Yubiry Gonzalez,  Ronaldo C. Prati, *Electronics 2022, 11, 1405*, https://www.mdpi.com/2079-9292/11/9/1405/htm

- [2] /Predominant Musical Instrument Classification based on Spectral Features./ Ankit Khairkar, Chaudhari Bhushan Jayant, Karthikeya Racharla, Paturu Harish, Vineet Kumar, *Indian Statistical Institute Kolkata, WB 700 108*,
https://arxiv.org/abs/1912.02606

- [3] /Spectral envelope estimation, representation, and morphing for sound analysis, transformation, and synthesis/, Diemo Schwarz, Xavier Rodet, *ICMC: International Computer Music Conference, Oct 1999, Pekin, China. pp.1-1.*
LINK={https://hal.science/hal-01161231}

- [4] /A CLASSIFICATION APPROACH TO MELODY TRANSCRIPTION./ Graham E. Poliner and Daniel P.W. Ellis, *LabROSA, Dept. of Electrical Engineering Columbia University, New York NY 10027 USA
2005 Queen Mary, University of London.*

- [5] /Music Similarity Measures: What’s the Use?/ Jean-Julien Aucouturier, Francois Pachet, *SONY Computer Science Lab. 6, rue Amyot 75005 Paris, France*

- [6] /TinySOL Database./ https://zenodo.org/record/3685367#.XnFp5i2h1IU%22

- [7] /VocalSet/ https://zenodo.org/record/1193957

- [8] /Google AudioSet/ http://research.google.com/audioset/

- [9] /Mel-frequency cepstrum/ https://en.wikipedia.org/wiki/Mel-frequency_cepstrum

- [10] /Melodic Contour/ https://phamoxmusic.com/melodic-contour/

- [11] /The NSynth Dataset/ https://magenta.tensorflow.org/datasets/nsynth

- [12] /Measure Similarity between two temporal signals/ https://en.wikipedia.org/wiki/Dynamic_time_warping

- [13] /REPET Algorithm/ https://lucainiaoge.github.io/2021/03/31/REPET_Algorithm_study/

- [14] /GuitarSet/ https://guitarset.weebly.com/

- [15] /Rhythm/ https://www.britannica.com/art/Baroque-music https://www.soundbrenner.com/blog/rhythm-basics-beat-measure-meter-time-signature-tempo/}

- [16] /Source Separation/ https://source-separation.github.io/tutorial/intro/tutorial_structure.html

- [17] /Chroma Feature Extraction/ https://www.researchgate.net/publication/330796993_Chroma_Feature_Extraction

- [18] /Cyclic tempogram/ https://ieeexplore.ieee.org/abstract/document/5495219/authors#authors

- [19] Mathematical reference example https://doi.org/10.1016/j.ejor.2022.05.008
