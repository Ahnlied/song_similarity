#+title: Methodology to determine song similarity
#+author: Jorge Andrés Carballo Santana

The main objective of this project is to extract non-trivial* features which describe a song. Ideally we can run this script with every song available, determine their score for every feature and, once we have a good database, create a song similarity score to recommend songs and create playlist with similar songs.


#* By trivial I mean parameters that doesn't have a formal musical justification and have a rather emotional or abstract definition (danceability, mood, energy, ...)



There are some parameters that characterizes a song, such as:

** 1. TONE COLOR/TIMBRE/INSTRUMENTATION
   - percussion = instr. which is struck, (drums, bell, rattle, marimba, vibes, xylophone, glockenspiel, piano (because note sounded by striking hammer))
   - strings (violin, viola, cello, double bass, guitar, harp etc.)
   - brass (trumpet, trombone, French horn, tuba, etc.)
   - woodwinds (flute, oboe, clarinet, bassoon, saxophone, etc. – last 4 = reeds, ob. & bsn. = double reeds vs. clar. & sax. = single reeds)
   - voices (soprano, alto, tenor, bass, etc. or the medieval triplum, duplum, tenor)
   - a.d.s.r. = attack-decay-sustain-release, a technical description of tone color, e.g. for synthesizer sound envelope design or analysis
     
** 2. RHYTHM
   - beat/pulse
   - meter = group of beats, e.g. group of four beats–"4/4"–numerator is number of beats
   - duple meter = a group of beats divisible by 2
   - triple meter = a group of beats divisible by 3
   - subdivision of individual beat can be duple or triple as well
   - a duple meter with triple subdivision is called compound duple meter and, vice versa, compound triple meter has duple subdivision

** 3. MELODY:
   - pitch + time = melody
   - pitch: high/low = sound wave, frequency of vibration of air on eardrum
   - melodic contour = a draw-able shape which shows perceived pitch height and rate of change
   - phrases–cadence–climax–scales (scales also pertain to harmony)


** 4. DYNAMICS/EXPRESSION:
   - loud/soft (forte/piano)
   - legato/staccato (smooth/choppy or spiky), etc.

** 5. TEXTURE (# OF INDEPENDENT PARTS):
   - monophony (one melody)
   - homophony (one main melody
   - polyphony (many simultaneous melodies which seem to relate)

** 6. HARMONY:
   - consonant/consonance (pleasing) vs. dissonant/dissonance (jarring, tense, ugly, unsettling)
   - cadence = ending, punctuation, as in speech
   - tonal system vs. modal or pre-tonal, atonal, microtonal or post-tonal
   - notes in scale or melody which play important harmonic roles
   - tonic (home note)
   - dominant (5 steps above tonic)
   - subdominant (5 steps below tonic)


** 7. FORM:
   - structure; music as architecture in space and time; musical similarity leads to symmetry
   - large-scale repetition and contrast diagrammed as rhyme scheme, e.g. 'ABA' represents a form which returns to the opening phrase before closing (also called an arch or da capo form)
   - strophic - 'AAA..etc.' like a song with verses, or a hymn
   - through-composed - a form where repetition is indiscernible or overly intricate


** 8. TEXT/EXTRAMUSICALITY:
   - use of words or other nonmusical ideas as essential part of piece vs. "absolute" music


* Approach:
The first approach is to develop either Machine Learning model or a numerical approach for each identification parameter of a song, such as, when executed each algorithm on a given song, the output would be a list of attributes given by each identification parameter. Our objective is to create a big database that contains the attributes of each
existing song.

The application of this model would be a recommendation system which brings fidelity to the musical taste of the user. The recommendation system would be built
by the intersection of attributes, and maybe other modifiable parameters like "range of similarity", the message transmitted by the song's lyrics and so on.

* 1. Develpoment TONE COLOR/TIMBRE/INSTRUMENTATION

#+begin_TONE COLOR/TIMBRE/INSTRUMENTATION
The approach that we followed to obtain this feature is to generate time windows and extract 13 Mel-Frequency Cepstrum Coefficients (MFCC), root mean square (rms), spectral centroid (spec_cent), spectral bandwidth (spec_bw), spectral rolloff, zero crossing rate (zcr). With these information we can detect to which instrument the past features correspond.

For each time window first we determine the type of instrument the prior corresponds, with that we determine the specific instrument. The instruments that we consider are separated in the type of instrument and the following:

   - Woodwind: Clarinet, Flute, Oboe, Bassoon, Alto Saxophone, Wind Instrument
   - Brass: Bass Tuba, French Horn, Trombone, Trumpet in C
   - Bowed string instruments: Cello, Viola, Violin, Bowed string instrument
   - Plucked string instruments: Guitar, Electric guitar, Acoustic guitar, Tapping, Bass guitar, Contrabass
   - Keyboard: Keyboard, Accordion
   - Percussion: Steelpan,Percussion, Drum and bass
   - Singing: Single Voice Singing, Group Singing
   - Noise: Noise

For these categories there exist an open-source database with single notes audio for each instrument (TinySOL).

Currently the result is a frequential count, but the objective is to create an statistical, probabilistic or a more complex result.

#+end_TONE COLOR/TIMBRE/INSTRUMENTATION


* 2. and 3. Develpoment RYTHM and MELODY:

#+begin_RYTHM/MELODY
To determine the melodic contour our approach is to, as the previous section, split the whole song in time windows and determine the pitch of each frame which will characterize the melodic contour.

Right now,  the melodic contour works as a mere finger print of the song; in the future we wish to determine melodic contour patterns which may describe it with less data and may work as a prior flag to determine a song similarity score before comparing fingerprints.

Also, it may be important to point out that the current melodic contour strategy has static frequencies (which range in the audible spectrum 200-4000Hz) which will describe the features. In the future it will be nice to stablish a range of dynamic relevant frequencies that constitute the signal and then split the relevant features taking these range into consideration (ex: an overall high pitched song should only have features ranging from 1050Hz or higher and despise low frequencies).






#+end_RYTHM/MELODY

* Important Links:
   * TinySOL Database - https://zenodo.org/record/3685367#.XnFp5i2h1IU%22
   * VocalSet - https://zenodo.org/record/1193957
   * Mel-frequency cepstrum - https://en.wikipedia.org/wiki/Mel-frequency_cepstrum
   * Reference article - https://www.mdpi.com/2079-9292/11/9/1405/htm
   * Mathematical reference example - https://doi.org/10.1016/j.ejor.2022.05.008
   * Feature from audio extraction reference - https://arxiv.org/abs/1912.02606
   * Melodic Contour - https://phamoxmusic.com/melodic-contour/
   * Measure Similarity between two temporal signals - https://en.wikipedia.org/wiki/Dynamic_time_warping
   * GuitarSet - https://guitarset.weebly.com/
   * Rhythm - https://www.britannica.com/art/Baroque-music https://www.soundbrenner.com/blog/rhythm-basics-beat-measure-meter-time-signature-tempo/
   * Source Separation - https://source-separation.github.io/tutorial/intro/tutorial_structure.html
   * REPET Algorithm - https://lucainiaoge.github.io/2021/03/31/REPET_Algorithm_study/
   * Chroma Feature Extraction - https://www.researchgate.net/publication/330796993_Chroma_Feature_Extraction
   * Cyclic tempogram - https://ieeexplore.ieee.org/abstract/document/5495219/authors#authors


* Modularity:
- Make a script that creates a data set of an input Youtube link. Make it so it can run in a terminal (this one is to run with the final model).
- Create another similar script which takes a reference Youtube link and append its output to the training dataset.

  
* Noise reduction:
- Find relations between original signal and noise. Simulate noise and add it to the single notes played.
  It would be nice to insolate and identify the instrument from a whole song an identify the signal-noise ratio.
- With this we would be able to isolate the peaks from the instrument from the background noise.
- Separate the song in parts: Background (repeating sounds which only stablishes rythm and tempo), Frontground (relevant sounds which helps on distinguish the melody of a song),
  also it would be important to retreave only singer's voices to make analysis on its characteristics. From this distinction it would be easier to extract the rythm of a song.
  

