#+title: Methodology to determine song similarity
#+author: Jorge Andrés Carballo Santana

There are some parameters that characterizes a song, such as:

** 1. TONE COLOR/TIMBRE/INSTRUMENTATION
   - percussion = instr. which is struck, (drums, bell, rattle, marimba, vibes, xylophone, glockenspiel, piano (because note sounded by striking hammer))
   - strings (violin, viola, cello, double bass, guitar, harp etc.)
   - brass (trumpet, trombone, French horn, tuba, etc.)
   - woodwinds (flute, oboe, clarinet, bassoon, saxophone, etc. – last 4 = reeds, ob. & bsn. = double reeds vs. clar. & sax. = single reeds)
   - voices (soprano, alto, tenor, bass, etc. or the medieval triplum, duplum, tenor)
   - a.d.s.r. = attack-decay-sustain-release, a technical description of tone color, e.g. for synthesizer sound envelope design or analysis
     
** 2. RHYTHM
   - beat/pulse
   - meter = group of beats, e.g. group of four beats–"4/4"–numerator is number of beats
   - duple meter = a group of beats divisible by 2
   - triple meter = a group of beats divisible by 3
   - subdivision of individual beat can be duple or triple as well
   - a duple meter with triple subdivision is called compound duple meter and, vice versa, compound triple meter has duple subdivision

** 3. MELODY:
   - pitch + time = melody
   - pitch: high/low = sound wave, frequency of vibration of air on eardrum
   - melodic contour = a draw-able shape which shows perceived pitch height and rate of change
   - phrases–cadence–climax–scales (scales also pertain to harmony)


** 4. DYNAMICS/EXPRESSION:
   - loud/soft (forte/piano)
   - legato/staccato (smooth/choppy or spiky), etc.

** 5. TEXTURE (# OF INDEPENDENT PARTS):
   - monophony (one melody)
   - homophony (one main melody
   - polyphony (many simultaneous melodies which seem to relate)

** 6. HARMONY:
   - consonant/consonance (pleasing) vs. dissonant/dissonance (jarring, tense, ugly, unsettling)
   - cadence = ending, punctuation, as in speech
   - tonal system vs. modal or pre-tonal, atonal, microtonal or post-tonal
   - notes in scale or melody which play important harmonic roles
   - tonic (home note)
   - dominant (5 steps above tonic)
   - subdominant (5 steps below tonic)


** 7. FORM:
   - structure; music as architecture in space and time; musical similarity leads to symmetry
   - large-scale repetition and contrast diagrammed as rhyme scheme, e.g. 'ABA' represents a form which returns to the opening phrase before closing (also called an arch or da capo form)
   - strophic - 'AAA..etc.' like a song with verses, or a hymn
   - through-composed - a form where repetition is indiscernible or overly intricate


** 8. TEXT/EXTRAMUSICALITY:
   - use of words or other nonmusical ideas as essential part of piece vs. "absolute" music

The objective of the current work is to create a project which detects and establish a "grade" of each parameter on a given song and, considering these grades, create playlists of similar vibes.


* Approach:
The first approach is to develop a Machine Learning model for each identification parameter of a song, such as, when executed each algorithm on a given song,
the output would be a list of attributes given by each identification parameter. Our objective is to create a big database that contains the attributes of each
existing song.

The application of this model would be a recommendation system which brings fidelity to the musical taste of the user. The recommendation system would be built
by the intersection of attributes, and maybe other modifiable parameters like "range of similarity", the message transmitted by the song's lyrics and so on.

* 1. Develpoment TONE COLOR/TIMBRE/INSTRUMENTATION


#+begin_TONE COLOR/TIMBRE/INSTRUMENTATION
The approach that I will follow to calculate this parameter is to find the instruments that integrate the song.
Maybe the most intuitive way to do this is to feed my database with the scale and sound of each existent instrument (or the most common used), this will be done by having an average tone and frequency of each note per instrument.
Since the most recent music is made by computer, I believe that we can still use the previous methodology, since a computer-generated sound may sound similar to an existing instrument.
#+end_TONE COLOR/TIMBRE/INSTRUMENTATION


* DIFICULTADES PARA EL DESARROLLO

Para definir un dataset con el que podamos trabajar, primero debemos definir la distribución de frecuencias y armónicos, así como su intensidad,  características de cada instrumento.
Para conseguir lo anterior se propone realizar un análisis espectral de sonido por medio de un espectrograma (transformada corta de Fourier) y contrastarlo con su respectivas frecuencias dadas por su transformada rápida de Fourier.

Idealmente se tendrían valores discretos de frecuencias y armónicos con su respectiva intensidad, pero en la práctica no se observa este comportamiento. Parece existir una distribución continua de frecuencias que describen cada sonido.
Para taclear lo anterior se intentará colapsar (o sumar)  la intensidad de las frecuencias circundantes a los picos (intensidades de frecuencias)  más pronunciados, esto para tener una base de datos con frecuencias discretas e intensidades variables (aunque también se puede plantear la idea de fracuencias discretas). 

Para alimentar al algoritmo de Machine Learning se debe idear una metodología con una cantidad de atributos (en este caso frecuencias y sus respectivas intensidades) discreto para que el algoritmo de Machine Learning (que aún no se establece cuál será) tenga un desempeño óptimo.
Establezcamos un espectro continuo con frecuencias enteras (no decimales).

¿Qué hacer con las frecuencias negativas?, ¿Qué significan (oscilación en sentido antihorario)?

Acotar las frecuencias permitidas (definir rango de escalas que nos interesan) B3 - D#5

Elaborar una base de datos y un modelo que detecte las notas aisladas de cada instrumento, posteriormente desarrollar uno con notas diferentes (una canción).

THE EXECUTION OF THE CLASSIFICATION MODEL HAS BEEN POSPONED SINCE CURRENTLY WE ARE EXPLORING OTHER FEATURES RELEVANT TO THE PROBLEM.

* Important Links:
TinySOL Database - https://zenodo.org/record/3685367#.XnFp5i2h1IU%22
VocalSet - https://zenodo.org/record/1193957
Reference article - https://www.mdpi.com/2079-9292/11/9/1405/htm
Mathematical reference example - https://doi.org/10.1016/j.ejor.2022.05.008
Feature from audio extraction reference - https://arxiv.org/abs/1912.02606
Melodic Contour - https://phamoxmusic.com/melodic-contour/
GuitarSet - https://guitarset.weebly.com/


* Modularity:
- Make a script that creates a data set of an input Youtube link. Make it so it can run in a terminal (this one is to run with the final model).
- Create another similar script which takes a reference Youtube link and append its output to the training dataset.

* Noise reduction:

- Find relations between original signal and noise. Simulate noise and add it to the single notes played.
  It would be nice to insolate and identify the instrument from a whole song an identify the signal-noise ratio.
- With this we would be able to isolate the peaks from the instrument from the background noise.
